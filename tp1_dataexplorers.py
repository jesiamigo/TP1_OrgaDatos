# -*- coding: utf-8 -*-
"""TP1 - DataExplorers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g59qBip4wf--XJSAPsD7NKrBPGyA0jRn

**Introducción:**

En el año 2015 Nepal fue afectado por el terremoto Gorkha, un sismo que registró una
magnitud de 7.8 en la escala Richter y tuvo su epicentro en la ciudad de Kathmandu.
Aproximadamente 600,000 estructuras en el centro y pueblos aledaños fueron dañadas o
destruidas. Un análisis posterior al sismo llevado por la Comisión Nacional de
Planeamiento de Nepal comunicó que la pérdida total económica ocasionada por el
terremoto fue de aproximadamente $7 mil millones (USD; NPC, 2015).
El dataset para el presente TP está compuesto de encuestas realizadas por Kathmandu
Living Labs y el Central Bureau of Statistics y contiene información sobre el impacto del
terremoto, estado de viviendas y estadísticas sociodemográficas.
Particularmente el dataset se enfoca en cómo eran las condiciones de una determinada
vivienda y cuál fue su grado de daño luego del accidente.
"""

# Commented out IPython magic to ensure Python compatibility.
# importacion general de librerias y de visualizacion (matplotlib y seaborn)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib
#plt.rcParams['figure.figsize'] = (20, 10)

sns.set(style="whitegrid") # seteando tipo de grid en seaborn

pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs

import warnings
warnings.filterwarnings('ignore')

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#Datos en https://www.drivendata.org/competitions/57/nepal-earthquake/data/
id='1Aj5sniiRHdXWetPNF6d36eMOREohvhNE'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('train_labels.csv')
train_labels = pd.read_csv('train_labels.csv')

id='19F4CRGMIuqQqcQ0zow0gvQcnelV7PQrH'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('train_values.csv')
train_values = pd.read_csv('train_values.csv')

train_labels.shape

train_values.shape

earthquakes = pd.merge(train_values, train_labels, how="inner", on="building_id")
earthquakes.head()

earthquakes.info()

earthquakes.dtypes

earthquakes.describe()

# antes de eliminar geo levels 2 y 3, nos fijamos cómo se distribuyen en geo level 1

earthquakes_by_geolevel1 = earthquakes.loc[:,['geo_level_1_id','geo_level_2_id','geo_level_3_id']].groupby('geo_level_1_id').nunique().reset_index(inplace=False)
earthquakes_by_geolevel1

# a partir del siguiente gráfico, podemos ver la distribución de los geo_level_2_id y geo_level_3_id  con respecto al geo_level_1_id 
# concluimos que poseen un comportamiento similar entre sí y que ninguno supera un orden de magnitud
# motivo por lo cual consideramos que no aportan información al análisis exploratorio de datos adicional a la que ya aporta geo_level_1_id 
earthquakes_by_geolevel1.iloc[:,[1,2]].plot(kind='line',logy=True, legend=True, )
plt.xlabel('geo_level_1_id') 
plt.ylabel('geo_level_2_id & geo_level_3_id') 
plt.title("Geo_level_1_id vs Geo_level_2_id and Geo_level_3_id", pad='15.0', fontweight='bold')
plt.show()

earthquakes = earthquakes.drop(columns = ['building_id', 'geo_level_2_id', 'geo_level_3_id'])

earthquakes = earthquakes.astype({
                    'geo_level_1_id': 'int8', \
                    'count_floors_pre_eq': 'int8', \
                    'age': 'int16', \
                    'area_percentage': 'int8', \
                    'height_percentage': 'int8', \
                    'count_families': 'int8', \
                    'land_surface_condition': 'category', \
                    'foundation_type': 'category', \
                    'roof_type': 'category', \
                    'ground_floor_type': 'category', \
                    'other_floor_type': 'category', \
                    'position': 'category', \
                    'plan_configuration': 'category', \
                    'legal_ownership_status': 'category', \
                    'damage_grade': 'int8', \
                    'has_superstructure_adobe_mud': 'bool', \
                    'has_superstructure_mud_mortar_stone': 'bool', \
                    'has_superstructure_stone_flag': 'bool', \
                    'has_superstructure_cement_mortar_stone': 'bool', \
                    'has_superstructure_mud_mortar_brick': 'bool', \
                    'has_superstructure_cement_mortar_brick': 'bool', \
                    'has_superstructure_timber': 'bool', \
                    'has_superstructure_bamboo': 'bool', \
                    'has_superstructure_rc_non_engineered': 'bool', \
                    'has_superstructure_rc_engineered': 'bool', \
                    'has_superstructure_other': 'bool', \
                    'has_secondary_use': 'bool', \
                    'has_secondary_use_agriculture': 'bool', \
                    'has_secondary_use_hotel': 'bool', \
                    'has_secondary_use_rental': 'bool', \
                    'has_secondary_use_institution': 'bool', \
                    'has_secondary_use_school': 'bool', \
                    'has_secondary_use_industry': 'bool', \
                    'has_secondary_use_health_post': 'bool', \
                    'has_secondary_use_gov_office': 'bool', \
                    'has_secondary_use_use_police': 'bool', \
                    'has_secondary_use_other': 'bool'\
                    })

earthquakes.dtypes

earthquakes.head()

earthquakes.describe()

#cantidad de terremotos en base al damage_grade
earthquakes_dg1 = earthquakes[earthquakes.damage_grade == 1].count()['damage_grade']
earthquakes_dg2 = earthquakes[earthquakes.damage_grade == 2].count()['damage_grade']
earthquakes_dg3 = earthquakes[earthquakes.damage_grade == 3].count()['damage_grade']  

total_dg = earthquakes_dg1 + earthquakes_dg2 + earthquakes_dg3
percentage_dg1 = round((earthquakes_dg1 * 100)/total_dg,2)
percentage_dg2 = round((earthquakes_dg2 * 100)/total_dg,2)
percentage_dg3 = round((earthquakes_dg3 * 100)/total_dg,2)

print('earthquakes with damage_grade = 1: ' + str(earthquakes_dg1) + ' = ' + str(percentage_dg1) + '%')
print('earthquakes with damage_grade = 2: ' + str(earthquakes_dg2) + ' = ' + str(percentage_dg2) + '%')
print('earthquakes with damage_grade = 3: ' + str(earthquakes_dg3) + ' = ' + str(percentage_dg3) + '%')

percentage_damage_grade = np.array([percentage_dg1, percentage_dg2, percentage_dg3])
labels = ['1: low damage (' + str(percentage_dg1) + '%)', \
          '2: medium amount of damage (' + str(percentage_dg2) + '%)',\
          '3: almost complete destruction (' + str(percentage_dg3) + '%)']

plt.pie(percentage_damage_grade, labels = labels)
plt.title("Damage grade de los terremotos", pad='15.0', fontweight='bold')
plt.show()

#se genera una copia de la base unificada para futuros filtros
# luego se realiza un gráfico de dispersion entre la cantidad de pisos y
# el porcentaje de altura para compararlas, a su vez esto para cada damage_grade
# y comparar sus correlaciones (se le aplicó un poco de fluctuación a los datos)

earthquakes2=earthquakes
lm = sns.lmplot(data=earthquakes, y='count_floors_pre_eq', x='height_percentage'\
          ,fit_reg=True, hue='damage_grade', legend=True, size=6,markers='.'\
          ,palette='Set2',aspect=.8,y_jitter=0.25,x_jitter=0.0,legend_out=True)
fig = lm.fig 
fig.suptitle("Relación entre height_percentage y count_floors_pre_eq", fontsize=12, fontweight='bold', y=1.05)

# una variante del grafico anterior para sumar, puede ser uno igual pero por cada damage, donde se categoriza por color los pisos.

lm = sns.lmplot(data=earthquakes, x='area_percentage', y='height_percentage', markers='.'\
           ,y_jitter=0.5,x_jitter=0.25,fit_reg=False\
           ,hue='count_floors_pre_eq',col_wrap=3,col='damage_grade')
fig = lm.fig 
fig.suptitle("height_percentage vs area_percentage analizado por separado en base al damaga_grade", fontsize=12, fontweight='bold', y=1.05)

# y para indagar algo raro en la distribucion de alturas por cada cantidad de pisos, lo hacemos por cada geo_level_1_id para ver el comportamiento

sns.lmplot(data=earthquakes, x='damage_grade', y='height_percentage', markers='.'\
           ,y_jitter=0.1,x_jitter=0.45,fit_reg=False,height=4, aspect=0.75\
           ,hue='count_floors_pre_eq',col_wrap=6,col='geo_level_1_id')

# se evaluó el campo geo_level_1_id vs el damage_grade
# con un grafico de barras apiladas de porcentajes (la suma da 100%)
# para esto se realiza un subdataset agrupando por geo_level_1_id y damage_grade
# se pasa cada valor de damage_grade como columnas
# y se agrega una suma total de registros por cada geo_level_1_id

subgeolevel1=earthquakes.groupby(['geo_level_1_id','damage_grade']).size().unstack()
subgeolevel1.columns=subgeolevel1.columns.get_level_values(0)
subgeolevel1.reset_index(inplace=True)
subgeolevel1.drop(columns='geo_level_1_id',inplace=True)
subgeolevel1['total']=subgeolevel1.iloc[:,[0,1,2]].sum(axis=1)
subgeolevel1.columns = ['damage_grade_1', 'damage_grade_2', 'damage_grade_3', 'total']

subgeolevel1

# luego se incorporan campos para porcentajes por cada damage_grade
# y algunos para ordenamiento de las barras en el gráfico y se plotea

subgeolevel1=subgeolevel1.assign(Damage_1 = lambda x: x['damage_grade_1']/x.total)
subgeolevel1=subgeolevel1.assign(Damage_2 = lambda x: x['damage_grade_2']/x.total)
subgeolevel1=subgeolevel1.assign(Damage_3 = lambda x: x['damage_grade_3']/x.total)
subgeolevel1=subgeolevel1.assign(Damage_1y3 = lambda x: x.Damage_1+x.Damage_3)
subgeolevel1.fillna(0,inplace=True)
subgeolevel1.sort_values(by=['Damage_3','Damage_1y3','Damage_2'],inplace=True)
plt.rcParams['figure.figsize'] = (7, 4)
subgeolevel1.iloc[:,[4,5,6]].plot(kind='bar',stacked=True)

plt.title("Proporción de damage_grade vs geo_level_1_id", pad=15, fontweight='bold')
plt.xlabel("geo_level_1_id")
plt.ylabel("damage_grade percentage")

# dada la relacion vista entre la cantidad de pisos y el porcentaje de altura,
# se determina utilizar la altura para un ratio con el area
# el mismo se genera como un nuevo campo en earthquakes2

earthquakes2['ratio_edificio']=earthquakes2['height_percentage']/earthquakes2['area_percentage']

# analizamos la relación entre damage_grade vs age a través de un scatter plot 

sc = sns.scatterplot(data=earthquakes, x='damage_grade', y='age', color='red')
plt.title("damage_grade vs age", fontsize=12, fontweight='bold')

# a partir del gráfico anterior, podríamos considerar que age = 995 es un outlier, así que podríamos eliminar del dataset las rows que correspondan a esa antiguedad. 
earthquakes_less_900 = earthquakes.loc[lambda earthquakes : earthquakes['age'] < 900]

sns.scatterplot(data=earthquakes_less_900, x='damage_grade', y='age', color='red')
plt.title("damage_grade vs age", fontsize=12, fontweight='bold')

# generamos histogramas de distribucion de datos de antiguedad
# con un doble filtro de antiguedad en el segundo plot

filtro_age_min=earthquakes2['age']>0
earthquakes3=earthquakes2[filtro_age_min]
filtro_age_max=earthquakes3['age']<=900
plt.rcParams['figure.figsize'] = (9, 5)
hp = sns.histplot(earthquakes3[filtro_age_max], x="age",stat='count'\
             ,binwidth=5,hue='damage_grade',multiple="stack",palette='Set1_r')

hp.set_title("Distribución de antiguedad de las edificaciones menores a 900 años vs cantidad", fontsize=12, fontweight='bold', y=1.05)

filtro_age_min=earthquakes2['age']>50
earthquakes3=earthquakes2[filtro_age_min]
filtro_age_max=earthquakes3['age']<=900
plt.rcParams['figure.figsize'] = (9, 5)
hp = sns.histplot(earthquakes3[filtro_age_max], x="age",stat='count'\
             ,binwidth=5,hue='damage_grade',multiple="stack",palette='Set1_r')

hp.set_title("Detalle de la distribución de antiguedad de las edificaciones entre 50 y 900 años vs cantidad en relación al damage_grade", fontsize=12, fontweight='bold', y=1.05)

#generamos dos filtros para quitar las antiguedades mayores a 900 años y los ratios mayores a 8

antiguedad900=earthquakes2['age']<=900
ratiosmenor8=earthquakes2['ratio_edificio']<=8

#aplicamos ambos filtros a earthquakes2

earthquakes2=earthquakes2[antiguedad900]
earthquakes2=earthquakes2[ratiosmenor8]
earthquakes2.describe()

# generamos un pack de scatter (uno por cada damage) de Ratio vs Age,
# donde los colores representan la variable categorica elegida (ej: ground_floor_type)

sns.lmplot(data=earthquakes2, y='age', x='ratio_edificio',fit_reg=False\
           , col='damage_grade', col_wrap=3,hue='land_surface_condition'\
           , legend=True,size=4,markers='.',palette='Set2',aspect=.8,y_jitter=2)
#plt.legend(loc='upper',title='ground_floor_type')

#generamos un pack de scatter (uno por cada damage) de Ratio vs Age,
# donde los colores representan la variable categorica elegida (ej: ground_floor_type)

sns.lmplot(data=earthquakes2, y='age', x='ratio_edificio',fit_reg=False\
           , col='damage_grade', col_wrap=3,hue='foundation_type'\
           , legend=True,size=4,markers='.',palette='Set2',aspect=.8,y_jitter=2)
#plt.legend(loc='upper',title='ground_floor_type')

#generamos un pack de scatter (uno por cada damage) de Ratio vs Age,
# donde los colores representan la variable categorica elegida (ej: ground_floor_type)

sns.lmplot(data=earthquakes2, y='age', x='ratio_edificio',fit_reg=False\
           , col='damage_grade', col_wrap=3,hue='roof_type'\
           , legend=True,size=4,markers='.',palette='Set2',aspect=.8,y_jitter=2)
#plt.legend(loc='upper',title='ground_floor_type')

#generamos un pack de scatter (uno por cada damage) de Ratio vs Age,
# donde los colores representan la variable categorica elegida (ej: ground_floor_type)

sns.lmplot(data=earthquakes2, y='age', x='ratio_edificio',fit_reg=False\
           , col='damage_grade', col_wrap=3,hue='ground_floor_type'\
           , legend=True,size=4,markers='.',palette='Set2',aspect=.8,y_jitter=2)
#plt.legend(loc='upper',title='ground_floor_type')

#generamos un pack de scatter (uno por cada damage) de Ratio vs Age,
# donde los colores representan la variable categorica elegida (ej: ground_floor_type)

sns.lmplot(data=earthquakes2, y='age', x='ratio_edificio',fit_reg=False\
           , col='damage_grade', col_wrap=3,hue='other_floor_type'\
           , legend=True,size=4,markers='.',palette='Set2',aspect=.8,y_jitter=2)
#plt.legend(loc='upper',title='ground_floor_type')

#generamos un pack de scatter (uno por cada damage) de Ratio vs Age,
# donde los colores representan la variable categorica elegida (ej: ground_floor_type)

sns.lmplot(data=earthquakes2, y='age', x='ratio_edificio',fit_reg=False\
           , col='damage_grade', col_wrap=3,hue='position'\
           , legend=True,size=4,markers='.',palette='Set2',aspect=.8,y_jitter=2)
#plt.legend(loc='upper',title='ground_floor_type')

#generamos un pack de scatter (uno por cada damage) de Ratio vs Age,
# donde los colores representan la variable categorica elegida (ej: ground_floor_type)

sns.lmplot(data=earthquakes2, y='age', x='ratio_edificio',fit_reg=False\
           , col='damage_grade', col_wrap=3,hue='plan_configuration'\
           , legend=True,size=4,markers='.',palette='Set2',aspect=.8,y_jitter=2)
#plt.legend(loc='upper',title='ground_floor_type')

earthquakes_by_floor_dg1 = earthquakes.loc[earthquakes['damage_grade'] == 1,['count_floors_pre_eq','damage_grade']].groupby(by=['count_floors_pre_eq']).count()
earthquakes_by_floor_dg2 = earthquakes.loc[earthquakes['damage_grade'] == 2,['count_floors_pre_eq','damage_grade']].groupby(by=['count_floors_pre_eq']).count()
earthquakes_by_floor_dg3 = earthquakes.loc[earthquakes['damage_grade'] == 3,['count_floors_pre_eq','damage_grade']].groupby(by=['count_floors_pre_eq']).count()

level0 = earthquakes_by_floor_dg1.columns.get_level_values(0)
earthquakes_by_floor_dg1.columns = level0
earthquakes_by_floor_dg1.reset_index(inplace=True)

level0 = earthquakes_by_floor_dg2.columns.get_level_values(0)
earthquakes_by_floor_dg2.columns = level0
earthquakes_by_floor_dg2.reset_index(inplace=True)

level0 = earthquakes_by_floor_dg3.columns.get_level_values(0)
earthquakes_by_floor_dg3.columns = level0
earthquakes_by_floor_dg3.reset_index(inplace=True)

earthquakes_by_floor = pd.merge(earthquakes_by_floor_dg1, earthquakes_by_floor_dg2, how="inner", on="count_floors_pre_eq")
earthquakes_by_floor = pd.merge(earthquakes_by_floor, earthquakes_by_floor_dg3, how="inner", on="count_floors_pre_eq")
earthquakes_by_floor = earthquakes_by_floor.rename(columns = {"damage_grade_x": "damage_grade_1", "damage_grade_y": "damage_grade_2", "damage_grade": "damage_grade_3"})
earthquakes_by_floor = earthquakes_by_floor.set_index('count_floors_pre_eq')
earthquakes_by_floor

earthquakes_by_floor.plot(kind="bar")
plt.title("Damage_grade vs count_floors_pre_eq", pad=15, fontweight='bold')
plt.xlabel("count_floors_pre_eq")
plt.ylabel("damage_grade")

# para ver si hay alguna tendencia entre las variables geo_level_1_id y count_floors_pre_eq, por ejemplo, si hay mayor cantidad de edificaciones con más pisos en alguna zona. 

def q25(x):
    return x.quantile(0.5)

def q75(x):
    return x.quantile(0.75)

earthquakes2.loc[:,['count_floors_pre_eq', 'damage_grade']].groupby(by=['count_floors_pre_eq']).agg({'damage_grade':[np.median, np.min, np.max, q25,q75]})

ax = sns.boxplot(x="count_floors_pre_eq", y="damage_grade", data=earthquakes2).set_title("Cantidad de pisos previo al terremoto vs el daño", fontsize=12, fontweight='bold', y=1.05)

# para analizar la relación entre la edad de la edificación y los pisos
ax = sns.boxplot(x="count_floors_pre_eq", y="age", data=earthquakes2).set_title("Cantidad de pisos previo al terremoto vs su edad", fontsize=12, fontweight='bold', y=1.05)

# analizamos la relación entre land_surface_condition con respecto a la antiguedad
ax = sns.boxplot(x="land_surface_condition", y="age", data=earthquakes2).set_title("Cantidad de pisos previo al terremoto vs land_surface_condition", fontsize=12, fontweight='bold', y=1.05)

ax = sns.boxplot(x="position", y="damage_grade", data=earthquakes2).set_title("position vs damage_grade", fontsize=12, fontweight='bold', y=1.05)

ax = sns.boxplot(x="plan_configuration", y="damage_grade", data=earthquakes2).set_title("damage_grade vs plan_configurarion", fontsize=12, fontweight='bold', y=1.05)

# analizamos legal_ownership_status vs damage_grade para ver si tienen alguna relación
ax = sns.boxplot(x="legal_ownership_status", y="damage_grade", data=earthquakes2).set_title("Cantidad de pisos previo al terremoto vs legal_ownership_status", fontsize=12, fontweight='bold', y=1.05)

#

sns.lmplot(data=earthquakes2, x='count_floors_pre_eq', y='count_families', x_jitter=0.25, y_jitter=0.25, hue='damage_grade', fit_reg=False, markers='.')
plt.title("count_floors_pre_eq vs count_families", fontsize=12, fontweight='bold')

# analizamos legal_ownership_status vs damage_grade para ver la distribución de los datos
ax = sns.boxplot(x="count_families", y="height_percentage", data=earthquakes2).set_title("cantidad de familias vs height_percentage", fontsize=12, fontweight='bold', y=1.05)

# analizamos legal_ownership_status vs damage_grade para ver si tienen alguna relación
ax = sns.boxplot(x="count_families", y="damage_grade", data=earthquakes2).set_title("Cantidad de familias vs damage_grade", fontsize=12, fontweight='bold', y=1.05)

# analizamos la relación entre la antiguedad y el material de construcción

earthquakes_by_construction_material = earthquakes2.loc[:,['age','has_superstructure_adobe_mud', \
                                                          'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag', \
                                                          'has_superstructure_cement_mortar_stone', 'has_superstructure_mud_mortar_brick', \
                                                          'has_superstructure_timber', 'has_superstructure_bamboo', \
                                                          'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', \
                                                          'has_superstructure_other', 'has_superstructure_cement_mortar_brick', 'damage_grade']]


earthquakes_by_construction_material

def asignarCodigo(df):
  newColumn = []
  for index,row in df.iterrows():
    newValue = ''
    if (row['has_superstructure_adobe_mud'] == 1):
      newValue += 'A'
    if (row['has_superstructure_mud_mortar_stone'] == 1):
      newValue += 'B'
    if (row['has_superstructure_stone_flag'] == 1):
      newValue += 'C'
    if (row['has_superstructure_cement_mortar_stone'] == 1):
      newValue += 'D'
    if (row['has_superstructure_mud_mortar_brick'] == 1):
      newValue += 'E'
    if (row['has_superstructure_cement_mortar_brick'] == 1):
      newValue += 'F'
    if (row['has_superstructure_timber'] == 1):
      newValue += 'G'
    if (row['has_superstructure_bamboo'] == 1):
      newValue += 'H'
    if (row['has_superstructure_rc_non_engineered'] == 1):
      newValue += 'I'
    if (row['has_superstructure_rc_engineered'] == 1):
      newValue += 'J'
    if (row['has_superstructure_other'] == 1):
      newValue += 'K'
    newColumn.append(newValue)
  return newColumn
       

materials = asignarCodigo(earthquakes_by_construction_material)

material_det = ['has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone','has_superstructure_stone_flag','has_superstructure_cement_mortar_stone',
                'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick','has_superstructure_timber','has_superstructure_bamboo',
                'has_superstructure_rc_non_engineered','has_superstructure_rc_engineered','has_superstructure_other'] 
letras = ['A','B','C','D','E','F','G','H','I','J','K']

detalle = {'material': material_det, 'letra': letras}
detalle = pd.DataFrame(data=detalle)
detalle

earthquakes_by_construction_material = earthquakes_by_construction_material.loc[:,['age', 'damage_grade']]
earthquakes_by_construction_material['materials'] = materials
earthquakes_by_construction_material

earthquakes_by_construction_material = earthquakes_by_construction_material.groupby(by=['materials']).agg({'age':np.mean, 'damage_grade':np.mean}).reset_index(inplace=False)
earthquakes_by_construction_material

earthquakes_by_construction_material_moreThan50 = earthquakes_by_construction_material[earthquakes_by_construction_material['age']>50]
earthquakes_by_construction_material_moreThan50

ax = sns.barplot(x='age', y='materials', data=earthquakes_by_construction_material_moreThan50).set_title("Distribución de materiales de acuerdo a la antiguedad mayor a 50 años", fontsize=12, fontweight='bold', y=1.05)

#con el objetivo de generar un HEATMAP 
#generamos un subdataframe solo con los campos boolean de MATERIALES CONSTRUCTIVOS
#renombramos las columnas con un prefijo para que no lo ordene alfabeticamente

boolmaterials=earthquakes.iloc[:,[12,13,14,15,16,17,18,19,20,21,22]]
boolmaterials.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                       '03_cement_mortar_stone','04_mud_mortar_brick',\
                       '05_cement_mortar_brick','06_timber','07_bamboo',\
                       '08_rc_non_engineered','09_rc_engineered','10_other']

# antes de agregarle un campo, generamos una serie con la cantidad de True por cada row
# y filtrando los registros con un solo True, los contamos por cada campo
# esto será usado para completar la diagonal del heatmap

seriediagonal=boolmaterials[boolmaterials[boolmaterials==True].count(axis=1)==1].sum()

# luego le agregamos un campo "contador" para sumar registros

boolmaterials['contador']=1

#iteramos entre los 11 campos boolean, para generar por cada uno,
# un pivot_table con la suma de cada match entre dicho campo y los otros 10

for i in range(11):
  filtro_p=boolmaterials[boolmaterials.columns.get_level_values(0)[i]]==True
  prueba_bool_concat=boolmaterials[filtro_p].\
                      pivot_table(index=boolmaterials.columns.\
                      get_level_values(0)[i],columns=['contador'],aggfunc=np.sum)
  if i==0:
    matrizfor=prueba_bool_concat
  else:
    matrizfor=pd.concat([matrizfor,prueba_bool_concat])

#limpiamos los indices y columnas

matrizfor.columns=matrizfor.columns.get_level_values(0)
matrizfor.reset_index(inplace=True)
matrizfor.drop(columns='index',inplace=True)

matrizfor

#reemplazamos los valores del indice, por los nombres de cada row, correspondientes a la columna que representan
#agregamos los datos de la seriediagonal en reemplazo de los valores "nan"

matrizfor['nombres']='abc'
for i in range(11):
  matrizfor['nombres'][i]=matrizfor.columns.get_level_values(0)[i]
  matrizfor[matrizfor.columns.get_level_values(0)[i]][i]=\
  seriediagonal[seriediagonal.index==matrizfor.columns.get_level_values(0)[i]]
matrizfor.set_index('nombres',drop=True, inplace=True)

#por último generamos el heatmap

g = sns.heatmap(matrizfor,linewidths=.1,annot=True,annot_kws={"size": 5},\
                cmap="gist_stern_r",fmt='g',robust=True,linecolor='gray')
g.set_title("cantidad de construcciones por materiales de construcción",fontsize=16)
g.set_xlabel("has_superstructure")
g.set_ylabel("has_superstructure")

# comenzamos de nuevo para un heatmap de medias de damage

boolmaterials_damage=earthquakes.iloc[:,[12,13,14,15,16,17,18,19,20,21,22,36]]
boolmaterials_damage.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                              '03_cement_mortar_stone','04_mud_mortar_brick',\
                              '05_cement_mortar_brick','06_timber','07_bamboo',\
                              '08_rc_non_engineered','09_rc_engineered','10_other','damage_grade']
for i in range(11):
  boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]=\
  boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]*\
  boolmaterials_damage['damage_grade']
boolmaterials_damage.drop(columns='damage_grade',inplace=True)

for i in range(11):
  filtro_p=boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]>0
  bool_merge=boolmaterials_damage[filtro_p].\
                      pivot_table(index=boolmaterials_damage.columns.\
                      get_level_values(0)[i],aggfunc={np.sum})
  bool_merge.columns=bool_merge.columns.get_level_values(0)
  serie_bool=bool_merge.sum()/matrizfor.loc[:,boolmaterials_damage.columns.get_level_values(0)[i]]
  if i==0:
    matriz_mean=serie_bool
  else:
    matriz_mean=pd.concat([matriz_mean,serie_bool],axis=1)
matriz_mean.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                              '03_cement_mortar_stone','04_mud_mortar_brick',\
                              '05_cement_mortar_brick','06_timber','07_bamboo',\
                              '08_rc_non_engineered','09_rc_engineered','10_other']

#por último generamos el heatmap

g = sns.heatmap(matriz_mean,linewidths=.1,annot=True,annot_kws={"size": 5},\
                cmap="RdYlBu_r",fmt='.3g',robust=False,linecolor='gray')
g.set_title("promedio de damage_grade por materiales de construcción",fontsize=16)
g.set_xlabel("has_superstructure")
g.set_ylabel("has_superstructure")

# analizamos si el hecho de que los edificios tengan un uso secundario afectó en el damage_grade
has_secondary_use_1 = earthquakes2.loc[earthquakes2['has_secondary_use'] == 1 ,['has_secondary_use','damage_grade']].groupby(by=['damage_grade']).count().reset_index(inplace=False)
has_secondary_use_1  = has_secondary_use_1.rename(columns={"has_secondary_use": "has_secondary_use_1"})
has_secondary_use_0 = earthquakes2.loc[earthquakes2['has_secondary_use'] == 0 ,['has_secondary_use','damage_grade']].groupby(by=['damage_grade']).count().reset_index(inplace=False)
has_secondary_use_0 = has_secondary_use_0.rename(columns={"has_secondary_use": "has_secondary_use_0"})

earthq_has_secondary_use = pd.merge(has_secondary_use_1, has_secondary_use_0, how="inner", on="damage_grade")
earthq_has_secondary_use

earthq_has_secondary_use.loc[:,['has_secondary_use_0','has_secondary_use_1']].plot(kind="bar")
plt.title("damage_grade vs has_secondary_use", pad=15, fontweight='bold')
plt.xlabel("damage_grade")
plt.ylabel("has_secondary_use")

has_secondary_use = earthquakes2.loc[earthquakes2['has_secondary_use'] == 1 ,['has_secondary_use']].count()
not_has_secondary_use = earthquakes2.loc[earthquakes2['has_secondary_use'] == 0 ,['has_secondary_use']].count()
total = has_secondary_use + not_has_secondary_use

has_secondary_use_perc = round(((has_secondary_use * 100)/total)[0],2)
not_has_secondary_use_perc = round(((not_has_secondary_use * 100)/total)[0],2)

print('has_secondary_use_perc = ' + str(has_secondary_use_perc) + '%')
print('not_has_secondary_use_perc = ' + str(not_has_secondary_use_perc) + '%')

percentage_secondary_use = np.array([has_secondary_use_perc, not_has_secondary_use_perc])
labels = ["has_secondary_use_perc (" + str(has_secondary_use_perc) + "%)", \
          "doesn't have secondary use (" + str(not_has_secondary_use_perc) + "%)"]

plt.pie(percentage_secondary_use, labels = labels)
plt.title("Has_secondary_use?", pad='15.0', fontweight='bold')
plt.show()

# Realizamos nuevamente el análisis por heatmaps de cantidades y de promedios,
# para los dos subsets de datos del gráfico anterior

# primero se generara el filtro para los heatmap con los que no tienen uso secundario
# se generará un subdataframe solo con los campos boolean de MATERIALES CONSTRUCTIVOS
# se renombraran las columnas con un prefijo para que no lo ordene alfabeticamente

filtroheatmaps=earthquakes['has_secondary_use']==0
boolmaterials=earthquakes[filtroheatmaps].iloc[:,[12,13,14,15,16,17,18,19,20,21,22]]
boolmaterials.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                       '03_cement_mortar_stone','04_mud_mortar_brick',\
                       '05_cement_mortar_brick','06_timber','07_bamboo',\
                       '08_rc_non_engineered','09_rc_engineered','10_other']

# antes de agregarle un campo, se genera una serie con la cantidad de True por cada row
# y filtrando los registros con un solo True, se cuentan los mismos por cada campo
# esto será usado para completar la diagonal del heatmap
# luego se agrega un campo "contador" para sumar registros

seriediagonal=boolmaterials[boolmaterials[boolmaterials==True].count(axis=1)==1].sum()
boolmaterials['contador']=1

# se itera entre los 11 campos boolean, para generar por cada uno,
# un pivot_table con la suma de cada match entre dicho campo y los otros 10

for i in range(11):
  filtro_p=boolmaterials[boolmaterials.columns.get_level_values(0)[i]]==True
  prueba_bool_concat=boolmaterials[filtro_p].\
                      pivot_table(index=boolmaterials.columns.\
                      get_level_values(0)[i],columns=['contador'],aggfunc=np.sum)
  if i==0:
    matrizfor=prueba_bool_concat
  else:
    matrizfor=pd.concat([matrizfor,prueba_bool_concat])

# se limpian los indices y columnas

matrizfor.columns=matrizfor.columns.get_level_values(0)
matrizfor.reset_index(inplace=True)
matrizfor.drop(columns='index',inplace=True)

# se reemplazan los valores del indice, por los nombres de cada row,
# correspondientes a la columna que representan
# se agregan los datos de la seriediagonal en reemplazo de los valores "nan"

matrizfor['nombres']='abc'
for i in range(11):
  matrizfor['nombres'][i]=matrizfor.columns.get_level_values(0)[i]
  matrizfor[matrizfor.columns.get_level_values(0)[i]][i]=\
  seriediagonal[seriediagonal.index==matrizfor.columns.get_level_values(0)[i]]
matrizfor.set_index('nombres',drop=True, inplace=True)

#por último se genera el heatmap

g = sns.heatmap(matrizfor,linewidths=.1,annot=True,annot_kws={"size": 5},\
                cmap="gist_stern_r",fmt='g',robust=True,linecolor='gray')
g.set_title("cantidad de construcciones por materiales de construcción para uso exclusivo como vivienda",fontsize=12, fontweight='bold')
g.set_xlabel("has_superstructure")
g.set_ylabel("has_superstructure")

# se comienza de nuevo para un heatmap de medias de damage
# para esto se usa el filtro anterior, dado que los totales para el promedio
# seran utilizados a partir de la "matrizfor" generada anteriormente

boolmaterials_damage=earthquakes[filtroheatmaps].iloc[:,[12,13,14,15,16,17,18,19,20,21,22,36]]
boolmaterials_damage.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                              '03_cement_mortar_stone','04_mud_mortar_brick',\
                              '05_cement_mortar_brick','06_timber','07_bamboo',\
                              '08_rc_non_engineered','09_rc_engineered','10_other','damage_grade']
for i in range(11):
  boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]=\
  boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]*\
  boolmaterials_damage['damage_grade']
boolmaterials_damage.drop(columns='damage_grade',inplace=True)

for i in range(11):
  filtro_p=boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]>0
  bool_merge=boolmaterials_damage[filtro_p].\
                      pivot_table(index=boolmaterials_damage.columns.\
                      get_level_values(0)[i],aggfunc={np.sum})
  bool_merge.columns=bool_merge.columns.get_level_values(0)
  serie_bool=bool_merge.sum()/matrizfor.loc[:,boolmaterials_damage.columns.get_level_values(0)[i]]
  if i==0:
    matriz_mean0=serie_bool
  else:
    matriz_mean0=pd.concat([matriz_mean0,serie_bool],axis=1)
matriz_mean0.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                              '03_cement_mortar_stone','04_mud_mortar_brick',\
                              '05_cement_mortar_brick','06_timber','07_bamboo',\
                              '08_rc_non_engineered','09_rc_engineered','10_other']

#por último se genera el heatmap de promedios

g = sns.heatmap(matriz_mean0,linewidths=.1,annot=True,annot_kws={"size": 5},\
                cmap="RdYlBu_r",fmt='.3g',robust=False,linecolor='gray')
g.set_title("promedio de damage_grade por materiales de construcción para uso exclusivo como vivienda",fontsize=12, fontweight='bold')
g.set_xlabel("has_superstructure")
g.set_ylabel("has_superstructure")

# Realizamos nuevamente el análisis por heatmaps de cantidades y de promedios,
# para los dos subsets de datos del gráfico anterior

# primero se generara el filtro para los heatmap con los que tienen uso secundario
# se generará un subdataframe solo con los campos boolean de MATERIALES CONSTRUCTIVOS
# se renombraran las columnas con un prefijo para que no lo ordene alfabeticamente

filtroheatmaps=earthquakes['has_secondary_use']==1
boolmaterials=earthquakes[filtroheatmaps].iloc[:,[12,13,14,15,16,17,18,19,20,21,22]]
boolmaterials.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                       '03_cement_mortar_stone','04_mud_mortar_brick',\
                       '05_cement_mortar_brick','06_timber','07_bamboo',\
                       '08_rc_non_engineered','09_rc_engineered','10_other']

# antes de agregarle un campo, se genera una serie con la cantidad de True por cada row
# y filtrando los registros con un solo True, se cuentan los mismos por cada campo
# esto será usado para completar la diagonal del heatmap
# luego se agrega un campo "contador" para sumar registros

seriediagonal=boolmaterials[boolmaterials[boolmaterials==True].count(axis=1)==1].sum()
boolmaterials['contador']=1

# se itera entre los 11 campos boolean, para generar por cada uno,
# un pivot_table con la suma de cada match entre dicho campo y los otros 10

for i in range(11):
  filtro_p=boolmaterials[boolmaterials.columns.get_level_values(0)[i]]==True
  prueba_bool_concat=boolmaterials[filtro_p].\
                      pivot_table(index=boolmaterials.columns.\
                      get_level_values(0)[i],columns=['contador'],aggfunc=np.sum)
  if i==0:
    matrizfor=prueba_bool_concat
  else:
    matrizfor=pd.concat([matrizfor,prueba_bool_concat])

# se limpian los indices y columnas

matrizfor.columns=matrizfor.columns.get_level_values(0)
matrizfor.reset_index(inplace=True)
matrizfor.drop(columns='index',inplace=True)

# se reemplazan los valores del indice, por los nombres de cada row,
# correspondientes a la columna que representan
# se agregan los datos de la seriediagonal en reemplazo de los valores "nan"

matrizfor['nombres']='abc'
for i in range(11):
  matrizfor['nombres'][i]=matrizfor.columns.get_level_values(0)[i]
  matrizfor[matrizfor.columns.get_level_values(0)[i]][i]=\
  seriediagonal[seriediagonal.index==matrizfor.columns.get_level_values(0)[i]]
matrizfor.set_index('nombres',drop=True, inplace=True)

#por último se genera el heatmap

g = sns.heatmap(matrizfor,linewidths=.1,annot=True,annot_kws={"size": 5},\
                cmap="gist_stern_r",fmt='g',robust=True,linecolor='gray')
g.set_title("cantidad de construcciones con uso secundario por materiales de construcción",fontsize=12, fontweight='bold')
g.set_xlabel("has_superstructure")
g.set_ylabel("has_superstructure")

# se comienza de nuevo para un heatmap de medias de damage
# para esto se usa el filtro anterior, dado que los totales para el promedio
# seran utilizados a partir de la "matrizfor" generada anteriormente

boolmaterials_damage=earthquakes[filtroheatmaps].iloc[:,[12,13,14,15,16,17,18,19,20,21,22,36]]
boolmaterials_damage.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                              '03_cement_mortar_stone','04_mud_mortar_brick',\
                              '05_cement_mortar_brick','06_timber','07_bamboo',\
                              '08_rc_non_engineered','09_rc_engineered','10_other','damage_grade']
for i in range(11):
  boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]=\
  boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]*\
  boolmaterials_damage['damage_grade']
boolmaterials_damage.drop(columns='damage_grade',inplace=True)

for i in range(11):
  filtro_p=boolmaterials_damage[boolmaterials_damage.columns.get_level_values(0)[i]]>0
  bool_merge=boolmaterials_damage[filtro_p].\
                      pivot_table(index=boolmaterials_damage.columns.\
                      get_level_values(0)[i],aggfunc={np.sum})
  bool_merge.columns=bool_merge.columns.get_level_values(0)
  serie_bool=bool_merge.sum()/matrizfor.loc[:,boolmaterials_damage.columns.get_level_values(0)[i]]
  if i==0:
    matriz_mean1=serie_bool
  else:
    matriz_mean1=pd.concat([matriz_mean1,serie_bool],axis=1)
matriz_mean1.columns=['00_adobe_mud','01_mud_mortar_stone','02_stone_flag',\
                              '03_cement_mortar_stone','04_mud_mortar_brick',\
                              '05_cement_mortar_brick','06_timber','07_bamboo',\
                              '08_rc_non_engineered','09_rc_engineered','10_other']

#por último se genera el heatmap de promedios

g = sns.heatmap(matriz_mean1,linewidths=.1,annot=True,annot_kws={"size": 5},\
                cmap="RdYlBu_r",fmt='.3g',robust=False,linecolor='gray')
g.set_title("promedio de damage_grade por materiales de construcción para construcciones con uso secundario",fontsize=12, fontweight='bold')
g.set_xlabel("has_superstructure")
g.set_ylabel("has_superstructure")

# ahora teniendo dos matrices de promedios de grado de daño,
# una para los correspondientes a edificaciones como vivienda exclusivamente
# y otra correspondiente a edificaciones con uso secundario
# las dividimos entre si para calcular porcentajes de una respecto de la otra

matriz_mean_dif=(matriz_mean1/matriz_mean0-1)*100

# y procedemos a hacer un heatmap del resultado

g = sns.heatmap(matriz_mean_dif,linewidths=.1,annot=True,annot_kws={"size": 7},\
                cmap="RdYlGn_r",fmt='.1f',robust=False,linecolor='gray',vmin=-15,vmax=15)
g.set_title("Porcentajes de diferencia para edificaciones con uso secundario respecto de las de uso exclusivo",fontsize=12, fontweight='bold')
g.set_xlabel("has_superstructure")
g.set_ylabel("has_superstructure")

earthquakes_by_other_uses = earthquakes2.loc[earthquakes2['has_secondary_use'] == 1]

earthquakes_by_other_uses = earthquakes_by_other_uses.loc[:,['has_secondary_use_agriculture', \
                                                'has_secondary_use_hotel', \
                                                'has_secondary_use_rental', \
                                                'has_secondary_use_institution',\
                                                'has_secondary_use_school', \
                                                'has_secondary_use_industry', \
                                                'has_secondary_use_health_post', \
                                                'has_secondary_use_gov_office', \
                                                'has_secondary_use_use_police', \
                                                'has_secondary_use_other', \
                                                'damage_grade']]
earthquakes_by_other_uses

# 
has_secondary_use_agriculture = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_agriculture'] == 1]['has_secondary_use_agriculture'].count()
has_secondary_use_hotel  = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_hotel'] == 1]['has_secondary_use_hotel'].count()
has_secondary_use_rental = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_rental'] == 1]['has_secondary_use_rental'].count()
has_secondary_use_institution = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_institution'] == 1]['has_secondary_use_institution'].count()
has_secondary_use_school = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_school'] == 1]['has_secondary_use_school'].count()
has_secondary_use_industry = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_industry'] == 1]['has_secondary_use_industry'].count()
has_secondary_use_health_post = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_health_post'] == 1]['has_secondary_use_health_post'].count()
has_secondary_use_gov_office = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_gov_office'] == 1]['has_secondary_use_gov_office'].count()
has_secondary_use_use_police = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_use_police'] == 1]['has_secondary_use_use_police'].count()
has_secondary_use_other = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_other'] == 1]['has_secondary_use_other'].count()

has_secondary_use_agriculture_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_agriculture'] == 1]['damage_grade'].mean()
has_secondary_use_hotel_mean  = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_hotel'] == 1]['damage_grade'].mean()
has_secondary_use_rental_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_rental'] == 1]['damage_grade'].mean()
has_secondary_use_institution_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_institution'] == 1]['damage_grade'].mean()
has_secondary_use_school_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_school'] == 1]['damage_grade'].mean()
has_secondary_use_industry_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_industry'] == 1]['damage_grade'].mean()
has_secondary_use_health_post_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_health_post'] == 1]['damage_grade'].mean()
has_secondary_use_gov_office_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_gov_office'] == 1]['damage_grade'].mean()
has_secondary_use_use_police_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_use_police'] == 1]['damage_grade'].mean()
has_secondary_use_other_mean = earthquakes_by_other_uses.loc[earthquakes_by_other_uses['has_secondary_use_other'] == 1]['damage_grade'].mean()

uses = ['has_secondary_use_agriculture', 'has_secondary_use_hotel', 'has_secondary_use_rental', 'has_secondary_use_institution', 'has_secondary_use_school', 'has_secondary_use_industry', 'has_secondary_use_health_post', 'has_secondary_use_gov_office', 'has_secondary_use_use_police', 'has_secondary_use_other']
counts = [has_secondary_use_agriculture, has_secondary_use_hotel, has_secondary_use_rental, has_secondary_use_institution, has_secondary_use_school, has_secondary_use_industry, has_secondary_use_health_post, has_secondary_use_gov_office, has_secondary_use_use_police, has_secondary_use_other]
means = [has_secondary_use_agriculture_mean, has_secondary_use_hotel_mean, has_secondary_use_rental_mean, has_secondary_use_institution_mean, has_secondary_use_school_mean, has_secondary_use_industry_mean, has_secondary_use_health_post_mean, has_secondary_use_gov_office_mean, has_secondary_use_use_police_mean, has_secondary_use_other_mean]
has_secondary_use_quantities = {'use': uses, 'count': counts, 'mean_damage_grade': means}
has_secondary_use_quantities = pd.DataFrame(data=has_secondary_use_quantities)
has_secondary_use_quantities

ax = sns.barplot(x='count', y='use', data=has_secondary_use_quantities).set_title("Distribución de las construcciones que tienen otros usos con antiguedad menores a 900 años", fontsize=12, fontweight='bold', y=1.05)

ax = sns.barplot(x='mean_damage_grade', y='use', data=has_secondary_use_quantities).set_title("Promedio de damage_grade para las construcciones que tienen otros usos con antiguedad menores a 900 años", fontsize=12, fontweight='bold', y=1.05)

#solo analizando los que tienen mas de 900 años de antiguedad 
earthquakes_by_other_uses_900 = earthquakes.loc[earthquakes['age'] >= 900]
earthquakes_by_other_uses_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use'] == 1]

earthquakes_by_other_uses_900 = earthquakes_by_other_uses_900.loc[:,['has_secondary_use_agriculture','has_secondary_use_hotel', 'has_secondary_use_rental', \
                                                                   'has_secondary_use_institution','has_secondary_use_school', 'has_secondary_use_industry', \
                                                                    'has_secondary_use_health_post', 'has_secondary_use_gov_office', 'has_secondary_use_use_police', \
                                                                    'has_secondary_use_other', 'damage_grade']]

has_secondary_use_agriculture_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_agriculture'] == 1]['has_secondary_use_agriculture'].count()
has_secondary_use_hotel_900  = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_hotel'] == 1]['has_secondary_use_hotel'].count()
has_secondary_use_rental_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_rental'] == 1]['has_secondary_use_rental'].count()
has_secondary_use_institution_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_institution'] == 1]['has_secondary_use_institution'].count()
has_secondary_use_school_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_school'] == 1]['has_secondary_use_school'].count()
has_secondary_use_industry_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_industry'] == 1]['has_secondary_use_industry'].count()
has_secondary_use_health_post_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_health_post'] == 1]['has_secondary_use_health_post'].count()
has_secondary_use_gov_office_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_gov_office'] == 1]['has_secondary_use_gov_office'].count()
has_secondary_use_use_police_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_use_police'] == 1]['has_secondary_use_use_police'].count()
has_secondary_use_other_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_other'] == 1]['has_secondary_use_other'].count()

has_secondary_use_agriculture_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_agriculture'] == 1]['damage_grade'].mean()
has_secondary_use_hotel_mean_900  = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_hotel'] == 1]['damage_grade'].mean()
has_secondary_use_rental_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_rental'] == 1]['damage_grade'].mean()
has_secondary_use_institution_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_institution'] == 1]['damage_grade'].mean()
has_secondary_use_school_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_school'] == 1]['damage_grade'].mean()
has_secondary_use_industry_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_industry'] == 1]['damage_grade'].mean()
has_secondary_use_health_post_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_health_post'] == 1]['damage_grade'].mean()
has_secondary_use_gov_office_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_gov_office'] == 1]['damage_grade'].mean()
has_secondary_use_use_police_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_use_police'] == 1]['damage_grade'].mean()
has_secondary_use_other_mean_900 = earthquakes_by_other_uses_900.loc[earthquakes_by_other_uses_900['has_secondary_use_other'] == 1]['damage_grade'].mean()

uses_900 = ['has_secondary_use_agriculture', 'has_secondary_use_hotel', 'has_secondary_use_rental', 'has_secondary_use_institution', 'has_secondary_use_school', 'has_secondary_use_industry', 'has_secondary_use_health_post', 'has_secondary_use_gov_office', 'has_secondary_use_use_police', 'has_secondary_use_other']
counts_900 = [has_secondary_use_agriculture_900, has_secondary_use_hotel_900, has_secondary_use_rental_900, has_secondary_use_institution_900, has_secondary_use_school_900, has_secondary_use_industry_900, has_secondary_use_health_post_900, has_secondary_use_gov_office_900, has_secondary_use_use_police_900, has_secondary_use_other_900]
means_900 = [has_secondary_use_agriculture_mean_900, has_secondary_use_hotel_mean_900, has_secondary_use_rental_mean_900, has_secondary_use_institution_mean_900, has_secondary_use_school_mean_900, has_secondary_use_industry_mean_900, has_secondary_use_health_post_mean_900, has_secondary_use_gov_office_mean_900, has_secondary_use_use_police_mean_900, has_secondary_use_other_mean_900]
has_secondary_use_quantities_900 = {'use': uses_900, 'count': counts_900, 'mean_damage_grade': means_900}
has_secondary_use_quantities_900 = pd.DataFrame(data=has_secondary_use_quantities_900)
has_secondary_use_quantities_900

ax = sns.barplot(x='count', y='use', data=has_secondary_use_quantities_900).set_title("Distribución de las construcciones que tienen otros usos con antiguedad mayor a 900 años", fontsize=12, fontweight='bold', y=1.05)

ax = sns.barplot(x='mean_damage_grade', y='use', data=has_secondary_use_quantities_900).set_title("Promedio de damage_grade de las construcciones que tienen otros usos con antiguedad mayores a 900 años", fontsize=12, fontweight='bold', y=1.05)